<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="de-DE" xml:lang="de-DE">
<head>
  <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
  <meta http-equiv="Content-Style-Type" content="text/css" />
  <meta name="generator" content="pandoc" />
  <meta name="author" content="Carsten Gips, FH Bielefeld" />
  <title>VL15 (BONUS): Naive Bayes</title>
  <style type="text/css">code{white-space: pre;}</style>
  <style type="text/css">
div.sourceCode { overflow-x: auto; }
table.sourceCode, tr.sourceCode, td.lineNumbers, td.sourceCode {
  margin: 0; padding: 0; vertical-align: baseline; border: none; }
table.sourceCode { width: 100%; line-height: 100%; }
td.lineNumbers { text-align: right; padding-right: 4px; padding-left: 4px; color: #aaaaaa; border-right: 1px solid #aaaaaa; }
td.sourceCode { padding-left: 5px; }
code > span.kw { color: #007020; font-weight: bold; } /* Keyword */
code > span.dt { color: #902000; } /* DataType */
code > span.dv { color: #40a070; } /* DecVal */
code > span.bn { color: #40a070; } /* BaseN */
code > span.fl { color: #40a070; } /* Float */
code > span.ch { color: #4070a0; } /* Char */
code > span.st { color: #4070a0; } /* String */
code > span.co { color: #60a0b0; font-style: italic; } /* Comment */
code > span.ot { color: #007020; } /* Other */
code > span.al { color: #ff0000; font-weight: bold; } /* Alert */
code > span.fu { color: #06287e; } /* Function */
code > span.er { color: #ff0000; font-weight: bold; } /* Error */
code > span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
code > span.cn { color: #880000; } /* Constant */
code > span.sc { color: #4070a0; } /* SpecialChar */
code > span.vs { color: #4070a0; } /* VerbatimString */
code > span.ss { color: #bb6688; } /* SpecialString */
code > span.im { } /* Import */
code > span.va { color: #19177c; } /* Variable */
code > span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code > span.op { color: #666666; } /* Operator */
code > span.bu { } /* BuiltIn */
code > span.ex { } /* Extension */
code > span.pp { color: #bc7a00; } /* Preprocessor */
code > span.at { color: #7d9029; } /* Attribute */
code > span.do { color: #ba2121; font-style: italic; } /* Documentation */
code > span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code > span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code > span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
  </style>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS_CHTML-full" type="text/javascript"></script>
  <!--
  Author: Carsten Gips <carsten.gips@fh-bielefeld.de>
  Copyright: (c) 2016 Carsten Gips
  License: MIT
  -->
  
  
  <style type="text/css">
  
  body {
      font-size: small;
      font-family: Optima, Arial, Helvetica, sans-serif;
      text-align: justify;
      margin: 0px; padding: 0px;
  }
  @media (min-width: 800px) {
  body {
      width: 700px; margin: 20px auto
  }
  }
  
  
  code { font-family: monospace; }
  
  h1 {
      font-size: 1.5em;
      font-weight: bold;
      color: #4070a0;
      text-align: left;
  }
  
  h2 {
      font-size: 1.2em;
      font-weight: bold;
      color: #4070a0;
      text-align: left;
  }
  
  h3 {
      font-size: 1.0em;
      font-weight: bold;
      color: #4070a0;
      text-align: left;
  }
  
  h4 {
      font-size: 1.0em;
      color: #4070a0;
      text-align: left;
  }
  
  h1 a, h2 a, h3 a, h4 a {
      text-decoration: none;
      color: #4070a0;
  }
  
  a {
      color: #4070a0;
  }
  
  /* Pandoc: title information */
  .title {
      font-size: 2.0em;
      font-weight: bold;
      color: #4070a0;
      text-align: left;
  }
  .subtitle {
      font-weight: normal;
      font-size: 1.0em;
      color: #4070a0;
      text-align: left;
  }
  .author {
      font-weight: normal;
      font-size: 1.0em;
      color: #000000;
      text-align: left;
  }
  .date {
      font-weight: normal;
      font-size: 1.0em;
      color: #000000;
      text-align: left;
  }
  #TOC {
      background-color: rgba(64, 112, 160, 0.1);
      padding: 10px 30px;
      text-align: left
  }
  
  /* definitions to match corresponding TeX macros */
  .alert { color: #ff3333; }
  .Alert { color: #ff3333; font-weight: bold; }
  .blueArrow { color: #4070a0; font-family: monospace; font-weight: bold; }
  .code { color: #00cc00; font-family: monospace; }
  .bsp { padding: 0.05cm; border-width: 0.05cm; border-style: solid; border-color: #4070a0; }
  .cbox { padding: 0.3cm; border-width: 0.1cm; border-style: solid; border-color: #4070a0; }
  
  </style>
</head>
<body>
<div id="header">
<h1 class="title">VL15 (BONUS): Naive Bayes</h1>
<h1 class="subtitle">IFM 5.6 Künstliche Intelligenz, 2017/18</h1>
<h2 class="author">Carsten Gips, FH Bielefeld</h2>
</div>
<div id="TOC">
<ul>
<li><a href="#wiederholung">Wiederholung</a></li>
<li><a href="#motivation">Motivation</a></li>
<li><a href="#grundlagen-der-wahrscheinlichkeitstheorie">Grundlagen der Wahrscheinlichkeitstheorie</a></li>
<li><a href="#klassifikation-mit-naive-bayes">Klassifikation mit Naive Bayes</a></li>
<li><a href="#zusammenfassung">Zusammenfassung</a></li>
</ul>
</div>
<h1 id="wiederholung">Wiederholung</h1>
<h2 id="erklären-sie-folgendes-programm">Erklären Sie folgendes Programm</h2>
<div class="sourceCode"><pre class="sourceCode prolog"><code class="sourceCode prolog">eating(dudley)<span class="kw">.</span>
happy(aunt_petunia) <span class="kw">:-</span> happy(dudley)<span class="kw">.</span>
happy(uncle_vernon) <span class="kw">:-</span> happy(dudley)<span class="kw">,</span> unhappy(harry)<span class="kw">.</span>
happy(dudley) <span class="kw">:-</span> kicking(dudley<span class="kw">,</span> harry)<span class="kw">.</span>
happy(dudley) <span class="kw">:-</span> eating(dudley)<span class="kw">.</span>
unhappy(harry) <span class="kw">:-</span> kicking(<span class="dt">_</span><span class="kw">,</span> harry)<span class="kw">.</span>
kicking(dudley<span class="kw">,</span> harry)<span class="kw">.</span>

<span class="fu">?-</span> happy(aunt_petunia)<span class="kw">.</span></code></pre></div>
<h1 id="motivation">Motivation</h1>
<h2 id="textklassifikation-mit-nb">Textklassifikation mit NB</h2>
<ul>
<li>Trainingsmenge:
<ul>
<li>D1: (“Sieben Zwerge fraßen sieben Ziegen”, 1)</li>
<li>D2: (“Sieben Ziegen traten sieben Wölfe”, 0)</li>
<li>D3: (“Sieben Wölfe fraßen sieben Böcke”, 1)</li>
<li>D4: (“Sieben Böcke traten sieben Zwerge”, 0)</li>
</ul></li>
<li>Testmenge:
<ul>
<li>T1: (“Sieben Zwerge fraßen sieben Wölfe”, 1)</li>
<li>T2: (“Sieben Zwerge traten sieben Ziegen”, 0)</li>
</ul></li>
</ul>
<p>Lernen Sie mit Hilfe der Trainingsmenge einen Naive-Bayes-Klassifikator und wenden Sie diesen auf die beiden Test-Dokumente an.</p>
<h2 id="themen-für-heute">Themen für heute</h2>
<p><img src="data:image/png;base64,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" alt="Mindmap" /><br />
</p>

<p>Am Ende der VL können/verstehen Sie:</p>

<ul>
<li>Grundlagen der Wahrscheinlichkeitstheorie</li>
<li>Klassifikation mit Naive Bayes</li>
</ul>
<h1 id="grundlagen-der-wahrscheinlichkeitstheorie">Grundlagen der Wahrscheinlichkeitstheorie</h1>
<h2 id="ereignisse-und-wahrscheinlichkeit">Ereignisse und Wahrscheinlichkeit</h2>

<p><strong>Hinweis</strong>: Die folgende Darstellung zur Einführung in die Wahrscheinlichkeitstheorie dient dem Verständnis des Naive Bayes Klassifikationsalgorithmus und ist teilweise eher oberflächlich gehalten. Sie kann und soll keine entsprechende mathematische Einführung ersetzen!</p>

<ul>
<li><p><strong>Ereignisse</strong> <span class="math inline">\(\Omega = \{\omega_1, \omega_2, ..., \omega_n\}\)</span>: endliche Menge der Ausgänge eines Versuchs</p></li>
<li><strong>Elementarereignis</strong>: Alle <span class="math inline">\(\omega_i \in \Omega\)</span>
<ul>
<li>decken <em>alle</em> möglichen Versuchsergebnisse ab, und</li>
<li>schließen sich gegenseitig aus</li>
</ul></li>
</ul>

<ul>
<li>Wenn <span class="math inline">\(A\)</span> und <span class="math inline">\(B\)</span> Ereignisse sind, dann auch <span class="math inline">\(A \cup B\)</span></li>
<li><span class="math inline">\(\Omega\)</span> wird als <strong>sicheres Ereignis</strong> bezeichnet: Enthält definitionsgemäß <strong>alle</strong> Versuchsausgänge, d.h. <em>ein</em> in der Menge enthaltenes Ereignis <em>muss</em> auftreten</li>
<li>Die leere Menge <span class="math inline">\(\emptyset\)</span> wird als <strong>unmögliches Ereignis</strong> bezeichnet.</li>
<li>Die Variablen <span class="math inline">\(A\)</span> und <span class="math inline">\(B\)</span> heißen auch <strong>Zufallsvariablen</strong>. Im Rahmen dieser Veranstaltung betrachten wir nur diskrete Zufallsvariablen mit endlichem Wertebereich!</li>
</ul>

<ul>
<li><p><strong>Wahrscheinlichkeit</strong>:</p>

<p>Sei <span class="math inline">\(\Omega = \{\omega_1, \omega_2, ..., \omega_n\}\)</span> endlich. Die Wahrscheinlichkeit <span class="math inline">\(P(A)\)</span> für ein Ereignis <span class="math inline">\(A\)</span> ist dann definiert als</p>

<p><span class="math display">\[
P(A) = \frac{|A|}{|\Omega|} =
\frac{\text{Anzahl der für A günstigen Fälle}}{\text{Anzahl der möglichen Fälle}}
\]</span></p></li>
</ul>

<ul>
<li><p>Man könnte auch schreiben: <span class="math inline">\(P(A) = \sum_{\omega \in A} P(\omega)\)</span></p>
<p><em>Hinweis</em>: Diese Definition von Wahrscheinlichkeit geht von einer gleichwahrscheinlichen Elementarereignissen aus! Die allgemeine Definition geht über einen entsprechenden Grenzwert.</p></li>
<li><p>Verteilung: Den Vektor mit den Wahrscheinlichkeiten aller Elementarereignisse nennt man auch <strong>Verteilung</strong></p>
<p>Beispiel: <span class="math inline">\(\mathbf{P}(A) = (P(A=1), P(A=2), ..., P(A=6)) = (1/6, 1/6, ..., 1/6)\)</span></p>
<p><em>Hinweis</em>: Wir betrachten hier nur diskrete Zufallsvariablen. Für kontinuierliche Variablen wird die Verteilung mit Hilfe einer Dichtefunktion dargestellt, beispielsweise der Gauss’schen Funktion.</p></li>
</ul>


<h3 id="beispiel">Beispiel</h3>
<ul>
<li><p>Einmaliges Würfeln mit einem Spielwürfel: <span class="math inline">\(\Omega = \{1,2,3,4,5,6\}\)</span></p></li>
<li><p>Elementarereignisse: <span class="math inline">\(\{1,2,3,4,5,6\}\)</span></p></li>
<li><p>Das Würfeln einer geraden Zahl (<span class="math inline">\(A = \{2,4,6\}\)</span>) ist <em>kein</em> Elementarereignis, ebenso wie das Würfeln einer Zahl kleiner 5 (<span class="math inline">\(B = \{1,2,3,4\}\)</span>), da <span class="math inline">\(A \cap B = \{2,4\}\)</span> $</p></li>
<li><p>Wahrscheinlichkeit, eine 1 zu würfeln: <span class="math inline">\(P(A \in \{1\}) = P(A=1) = \frac{1}{6}\)</span>.</p>
<p><em>Anmerkung</em>: Man schreibt statt <span class="math inline">\(P(A \in \{1\})\)</span> oft einfach <span class="math inline">\(P(1)\)</span>.</p></li>
<li><p>Wahrscheinlichkeit, eine gerade Zahl zu würfeln:</p>
<p><span class="math inline">\(P(A \in \{2,4,6\}) = P(A=2 \vee A=4 \vee A=6) = \frac{|\{2,4,6\}|}{|\{1,2,3,4,5,6\}|} = \frac{3}{6} = 0.5\)</span></p></li>
</ul>

<p><span class="bsp">Tafel: Würfeln</span> </p>
<h2 id="rechenregeln">Rechenregeln</h2>

<p><strong>Kolmogorov Axiome</strong></p>

<p>Sei <span class="math inline">\(A\)</span> ein Event, also <span class="math inline">\(A \subseteq \Omega\)</span>:</p>

<ul>
<li><p><span class="math inline">\(0 \le P(A) \le 1\)</span></p></li>
<li><p><span class="math inline">\(\Omega = \{\omega_1, \omega_2, ..., \omega_n\}\)</span>: <span class="math inline">\(\sum_{i} P(\omega_i) = 1\)</span> (Normierungsbedingung: Summe über die Wahrscheinlichkeiten aller Elementarereignisse ist immer 1) </p></li>
<li><p><span class="math inline">\(P(A \cup B) = P(A) + P(B) - P(A \cap B)\)</span></p></li>
</ul>

<p>Daraus folgt u.a.:</p>
<ul>
<li><p><span class="math inline">\(P(\Omega) = 1\)</span></p></li>
<li><p><span class="math inline">\(P(\emptyset) = 0\)</span></p></li>
<li><p><span class="math inline">\(P(A) = 1- P(\neg A)\)</span></p></li>
<li><p><span class="math inline">\(A\)</span> und <span class="math inline">\(B\)</span> <em>unabhängig</em>: <span class="math inline">\(P(A \cup B) = P(A) + P(B)\)</span></p>

<p><span class="math inline">\(P(A \cap B)\)</span> ist leer, wenn <span class="math inline">\(A\)</span> und <span class="math inline">\(B\)</span> sich nicht überlappen</p>
</li>
<li><p><span class="math inline">\(A \subseteq B\)</span>: <span class="math inline">\(P(A) \le P(B)\)</span></p></li>
</ul>
<p><span class="bsp">Tafel: Würfeln: Elementar, gerade Zahl</span> </p>
<h2 id="verbundwahrscheinlichkeiten">Verbundwahrscheinlichkeiten</h2>
<p><span class="math display">\[P(A,B) = P(B,A) = \text{ Wahrscheinlichkeit, dass A und B gleichzeitig auftreten }\]</span></p>

<table>
<thead>
<tr class="header">
<th align="left"></th>
<th align="left">Zahnschmerzen</th>
<th align="left"><span class="math inline">\(\neg\)</span> Zahnschmerzen</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">Karies</td>
<td align="left">0.04</td>
<td align="left">0.06</td>
</tr>
<tr class="even">
<td align="left"><span class="math inline">\(\neg\)</span> Karies</td>
<td align="left">0.01</td>
<td align="left">0.89</td>
</tr>
</tbody>
</table>
<ul>
<li><span class="math inline">\(P(K,Z) = 0.04\)</span></li>
</ul>
<h2 id="bedingte-wahrscheinlichkeit">Bedingte Wahrscheinlichkeit</h2>

<p><strong>Definition</strong>: Bedingte Wahrscheinlichkeit für <span class="math inline">\(A\)</span> gegeben <span class="math inline">\(B\)</span>:</p>

<p><span class="math display">\[P(A | B) = \frac{P(A,B)}{P(B)}\]</span></p>

<table>
<thead>
<tr class="header">
<th align="left"></th>
<th align="left">Zahnschmerzen</th>
<th align="left"><span class="math inline">\(\neg\)</span> Zahnschmerzen</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">Karies</td>
<td align="left">0.04</td>
<td align="left">0.06</td>
</tr>
<tr class="even">
<td align="left"><span class="math inline">\(\neg\)</span> Karies</td>
<td align="left">0.01</td>
<td align="left">0.89</td>
</tr>
</tbody>
</table>
<ul>
<li><p><span class="math inline">\(P(\text{Karies} | \text{Zahnschmerzen}) = \frac{P(K,Z)}{P(Z)} = \frac{0.04}{0.04+0.01} = 0.8\)</span></p></li>
<li><p><span class="math inline">\(P(\text{Zahnschmerzen} | \text{Karies}) = \frac{P(Z,K)}{P(K)} = \frac{0.04}{0.04+0.06} = 0.4\)</span></p></li>
</ul>

<p>Wegen <span class="math inline">\(P(A | B) = \dfrac{P(A,B)}{P(B)}\)</span> ist <span class="math inline">\(P(A,B) = P(A|B)P(B) = P(B|A)P(A)\)</span> (<strong>Produkt-Regel</strong>)!</p>

<h2 id="marginalisierung">Marginalisierung</h2>
<table>
<thead>
<tr class="header">
<th align="left"></th>
<th align="left">Zahnschmerzen</th>
<th align="left"><span class="math inline">\(\neg\)</span> Zahnschmerzen</th>
<th align="left"><em>Summe</em></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">Karies</td>
<td align="left">0.04</td>
<td align="left">0.06</td>
<td align="left"><em>0.1</em></td>
</tr>
<tr class="even">
<td align="left"><span class="math inline">\(\neg\)</span> Karies</td>
<td align="left">0.01</td>
<td align="left">0.89</td>
<td align="left"><em>0.9</em></td>
</tr>
<tr class="odd">
<td align="left"><em>Summe</em></td>
<td align="left"><em>0.05</em></td>
<td align="left"><em>0.95</em></td>
<td align="left"><em>1</em></td>
</tr>
</tbody>
</table>
<p><span class="math inline">\(P(K) = P(K,Z) + P(K, \neg Z)\)</span></p>
<p></p>
<p>Allgemein: Seien <span class="math inline">\(B_1, ..., B_n\)</span> Elementarereignisse mit <span class="math inline">\(\bigcup_i B_i = \Omega\)</span>. Dann ist <span class="math display">\[P(A) = \sum_i P(A,B_i) = \sum_i P(A|B_i)P(B_i)\]</span></p>

<p>Diesen Vorgang nennt man <strong>Marginalisierung</strong>. Die resultierende Verteilung <span class="math inline">\(P(A)\)</span> nennt man auch <em>“Randverteilung”</em>, da sie mit einer Projektion eines Quaders auf eine Seitenfläche vergleichbar ist.</p>

<h2 id="kettenregel">Kettenregel</h2>
<ul>
<li><strong>Produktregel</strong>: Wegen <span class="math inline">\(P(A|B) = \dfrac{P(A,B)}{P(B)}\)</span> gilt <span class="math inline">\(P(A,B) = P(A|B)P(B)\)</span></li>
</ul>

<ul>
<li>Verallgemeinerung (<strong>Kettenregel</strong>): <span class="math display">\[
\begin{array}{rcl}
P(A_1,A_2,...,A_n) &amp;=&amp; P(A_n,...,A_2,A_1)\\
    &amp; = &amp; P(A_n|A_{n-1},...,A_1)P(A_{n-1},...,A_1)\\
    &amp; = &amp; P(A_n|A_{n-1},...,A_1)P(A_{n-1}|A_{n-2},...,A_1)P(A_{n-2},...,A_1)\\
    &amp; = &amp; \text{ ... }\\
    &amp; = &amp; P(A_n|A_{n-1},...,A_1)\text{ ... }P(A_2|A_1)P(A_1)\\
    &amp; = &amp; \prod_i P(A_i|A_1,...,A_{i-1})
\end{array}
\]</span></li>
</ul>
<h2 id="bayes-regel">Bayes Regel</h2>
<p>Bedingte Wahrscheinlichkeit: <span class="math inline">\(P(A,B) = P(A|B)P(B) = P(B|A)P(A)\)</span></p>

<p><span class="math display">\[
P(A|B) = \frac{P(B|A)P(A)}{P(B)}
\]</span></p>

<ul>
<li><p><span class="math inline">\(P(A)\)</span> nennt man auch <strong>“Prior”</strong> oder <strong>“A-priori-Wahrscheinlichkeit”</strong>: Das ist die Wahrscheinlichkeit für <span class="math inline">\(A\)</span> ohne weiteres Wissen</p></li>
<li><p><span class="math inline">\(P(B|A)\)</span> nennt man auch <strong>“Likelihood”</strong>: Wie wahrscheinlich ist das Auftreten von <span class="math inline">\(B\)</span>, gegeben <span class="math inline">\(A\)</span>?</p></li>
<li><p><span class="math inline">\(P(A|B)\)</span> nennt man auch <strong>“Posterior”</strong> oder <strong>“A-posteriori-Wahrscheinlichkeit”</strong>: Wie wahrscheinlich ist <span class="math inline">\(A\)</span>, wenn <span class="math inline">\(B\)</span> eingetreten ist?</p></li>
<li><p><span class="math inline">\(P(B)\)</span> ist ein Normierungsfaktor.</p></li>
</ul>
<p>Wenn man (siehe später: Naive Bayes Klassifikator) <span class="math inline">\(A\)</span> als Klasse und <span class="math inline">\(B\)</span> als Daten betrachtet:</p>
<ul>
<li><span class="math inline">\(P(A)\)</span>: Wie wahrscheinlich ist eine bestimmte Klasse an sich (A-priori-Wahrscheinlichkeit der Klassen)?</li>
<li><span class="math inline">\(P(B|A)\)</span>: Wie wahrscheinlich sind bestimmte Daten, gegeben die Klasse <span class="math inline">\(A\)</span>? (Likelihood der Daten)</li>
<li><span class="math inline">\(P(A|B)\)</span>: Gegeben die Daten <span class="math inline">\(B\)</span>, wie wahrscheinlich ist die Klasse <span class="math inline">\(A\)</span>? (Posterior)</li>
</ul>
<p>In der Medizin hat sucht man i.d.R. die Ursache für beobachtete Symptome: <span class="math display">\[
P(\text{Ursache}|\text{Symptome}) = \frac{P(\text{Symptome}|\text{Ursache})P(\text{Ursache})}{P(\text{Symptome})}
\]</span></p>
<p>Aus der A-priori-Wahrscheinlichkeit für bestimmte Krankheiten und der Likelihood der Symptome (wie wahrscheinlich sind Symptome, gegeben eine Krankheit) kann man die Wahrscheinlichkeit für das Vorliegen einer Erkrankung gegeben bestimmte Symptome berechnen.</p>

<h2 id="beispiel-bayes">Beispiel Bayes</h2>
<ul>
<li>Bei Meningitis wird oft ein steifer Hals beobachtet: <span class="math inline">\(P(S|M) = 0.8\)</span></li>
<li>Eine von 10000 Personen hat Meningitis: <span class="math inline">\(P(M) = 0.0001\)</span></li>
<li>Eine von 10 Personen hat einen steifen Hals: <span class="math inline">\(P(S) = 0.1\)</span></li>
</ul>
<p><span class="blueArrow">=&gt;</span> Ich habe einen steifen Hals. Habe ich Meningitis?</p>

<p><span class="math display">\[
P(M|S) = \frac{P(S|M)P(M)}{P(S)} = \frac{0.8 \times 0.0001}{0.1} = 0.0008 = 0.08\%
\]</span></p>

<p>Bei einem steifen Hals liegt die Wahrscheinlichkeit, an Meningitis erkrankt zu sein, bei nur 0.08%. Kein Grund zur Sorge in diesem Fall :-)</p>

<h2 id="beispiel-bayes-ii">Beispiel Bayes II</h2>

<p>Beispiel aus Wolfgang Ertel: “Grundkurs Künstliche Intelligenz”, Springer Vieweg, 2016 (Seite 148)</p>

<ul>
<li>Alarmanlage meldet jeden Einbruch mit einer Sicherheit von 99%</li>
<li>Wahrscheinlichkeit für Einbruch: 0.1%</li>
<li>Wird auch von Tieren o.ä. ausgelöst, Alarm-Wahrscheinlichkeit 10%</li>
</ul>
<p><span class="blueArrow">=&gt;</span> Wie wahrscheinlich ist ein Einbruch, wenn der Alarm ertönt?</p>

<ul>
<li>Gegeben: <span class="math inline">\(P(A) = 0.1, \; P(E) = 0.001, \; P(A|E) = 0.99\)</span></li>
<li>Gesucht: <span class="math inline">\(P(E|A)\)</span></li>
</ul>

<p><span class="math display">\[
P(E|A) = \frac{P(A|E)P(E)}{P(A)} = \frac{0.99 \times 0.001}{0.1} = 0.0099 = 0.99\%
\]</span></p>

<p>Bei Ertönen des Alarms liegt die Wahrscheinlichkeit für einen Einbruch bei nur knapp 1 Prozent. Diese Alarmanlage ist vielleicht nicht sehr hilfreich …</p>


<p>Frage: Wie wahrscheinlich ist ein Alarm ohne Einbruch, also <span class="math inline">\(P(A|\neg E\)</span>)?</p>

<p>Mit Marginalisierung: <span class="math inline">\(P(A) = P(A|E)P(E) + P(A|\neg E)P(\neg E)\)</span>, d.h. <span class="math inline">\(0.1 = 0.99 \times 0.001 + P(A|\neg E) \times (1-0.001) = 0.00099 + P(A|\neg E) \times 0.999\)</span></p>
<p><span class="blueArrow">=&gt;</span> <span class="math inline">\(P(A|\neg E) = 0,0991\)</span></p>

<p>In knapp 10 Prozent der Fälle wird der Alarm ohne Einbruch ausgelöst …</p>

<h2 id="unabhängige-ereignisse">Unabhängige Ereignisse</h2>
<ul>
<li><p><span class="math inline">\(P(\text{Zahnschmerzen},\text{Regen}) = P(\text{Regen}|\text{Zahnschmerzen})P(\text{Zahnschmerzen})\)</span></p></li>
<li><p><span class="math inline">\(P(\text{Regen}|\text{Zahnschmerzen}) = \text{ ? }\)</span> <span class="math inline">\(= P(\text{Regen})\)</span></p></li>
</ul>
<p></p>
<ul>
<li><p>Zwei Ereignisse <span class="math inline">\(A\)</span> und <span class="math inline">\(B\)</span> sind <strong>unabhängig</strong>, wenn <span class="math display">\[ P(A|B) = P(A) \]</span></p>

<p><span class="blueArrow">=&gt;</span> <span class="math inline">\(P(A,B) = P(A|B)P(B) = P(A)P(B)\)</span></p></li>
</ul>

<ul>
<li><p>Allgemein (<strong>bedingte Unabhängigkeit</strong>):</p>

<p><span class="math inline">\(X\)</span> und <span class="math inline">\(Y\)</span> sind <em>bedingt unabhängig</em> (gegeben <span class="math inline">\(Z\)</span>), wenn <span class="math inline">\(P(X|Y,Z) = P(X|Z)\)</span> bzw. <span class="math inline">\(P(Y|X,Z) = P(Y|Z)\)</span></p>
<p>Daraus folgt:</p>

<p><span class="math display">\[ P(X,Y|Z) = P(X|Y,Z)P(Y|Z) = P(X|Z)P(Y|Z) \]</span></p></li>
</ul>
<h1 id="klassifikation-mit-naive-bayes">Klassifikation mit Naive Bayes</h1>
<h2 id="naive-bayes">Naive Bayes</h2>
<ul>
<li><p>Verallgemeinerte Bayes Regel <span class="math display">\[
P(H|D_1, ..., D_n) = \frac{P(D_1, ..., D_n|H)P(H)}{P(D_1, ..., D_n)}
\]</span></p></li>
<li><p>Annahme: <span class="math inline">\(D_i\)</span> sind bedingt unabhängig <span class="math display">\[
P(D_1, ..., D_n|H) = P(D_1|H) \cdot ... \cdot P(D_n|H) = \prod_i P(D_i|H)
\]</span></p></li>
<li><p>Beobachtung: <span class="math inline">\(P(D_1, ..., D_n)\)</span> für alle Hypothesen <span class="math inline">\(h \in H\)</span> gleich</p></li>
</ul>

<ul>
<li><p><strong>Naive Bayes Klassifikator</strong> bzw. <strong>MAP</strong> (“Maximum a Posteriori”) <span class="math display">\[
h_{MAP} = \operatorname{argmax}_{h \in H} P(h|D_1, ..., D_n)
= \operatorname{argmax}_{h \in H} P(h) \prod_i P(D_i|h)
\]</span></p>

<p>Naive Bayes: Wähle die plausibelste Hypothese, die von den Daten unterstützt wird.</p>
</li>
</ul>
<h2 id="bayessches-lernen">Bayes’sches Lernen</h2>
<p><span class="math display">\[
h_{MAP} = \operatorname{argmax}_{h \in H} P(h|D_1, ..., D_n)
= \operatorname{argmax}_{h \in H} P(h) \prod_i P(D_i|h)
\]</span></p>

<p><strong>Training</strong>: Bestimme die Wahrscheinlichkeiten aus Trainingsdaten <span class="math inline">\(\mathbf{S}\)</span></p>
<ul>
<li><p>Für jede Klasse <span class="math inline">\(h\)</span>:</p>
<ul>
<li><p>Schätze <span class="math inline">\(P(h) = \dfrac{|S(h)|}{|S|}\)</span></p></li>
<li><p>Für jedes Attribut <span class="math inline">\(D_i\)</span> und jede Ausprägung <span class="math inline">\(x \in D_i\)</span>:</p>
<p>Schätze <span class="math inline">\(P(D_i=x|h) = \dfrac{|S_{D_i}(x) \cap S(h)|}{|S(h)|}\)</span></p></li>
</ul></li>
</ul>

<p><strong>Klassifikation</strong>: Wähle wahrscheinlichste Klasse <span class="math inline">\(h_{MAP}\)</span> für Vektor <span class="math inline">\(\mathbf{x}\)</span></p>
<ul>
<li><span class="math inline">\(h_{MAP} = \operatorname{argmax}_{h \in H} P(h) \prod_{x \in \mathbf{x}} P(x|h)\)</span></li>
</ul>
<h2 id="beispiel-nb">Beispiel NB</h2>

<table>
<thead>
<tr class="header">
<th align="left">Beispiel</th>
<th align="left">Himmel</th>
<th align="left">Lufttemp.</th>
<th align="left">Wassertemp.</th>
<th align="left">Wind</th>
<th align="left">Schwimmen?</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">1</td>
<td align="left">sonnig</td>
<td align="left">warm</td>
<td align="left">warm</td>
<td align="left">windstill</td>
<td align="left">ja</td>
</tr>
<tr class="even">
<td align="left">2</td>
<td align="left">sonnig</td>
<td align="left">kalt</td>
<td align="left">warm</td>
<td align="left">stürmisch</td>
<td align="left">nein</td>
</tr>
<tr class="odd">
<td align="left">3</td>
<td align="left">sonnig</td>
<td align="left">warm</td>
<td align="left">warm</td>
<td align="left">brise</td>
<td align="left">ja</td>
</tr>
<tr class="even">
<td align="left">4</td>
<td align="left">bewölkt</td>
<td align="left">kalt</td>
<td align="left">kalt</td>
<td align="left">windstill</td>
<td align="left">nein</td>
</tr>
<tr class="odd">
<td align="left">5</td>
<td align="left">regen</td>
<td align="left">kalt</td>
<td align="left">warm</td>
<td align="left">windstill</td>
<td align="left">ja</td>
</tr>
</tbody>
</table>
<ul>
<li>Eingabe: Merkmalsvektor <code>(sonnig, kalt, warm, windstill)</code></li>
</ul>
<p></p>
<p>Gesucht: <span class="math inline">\(P(\text{ja})\)</span>, <span class="math inline">\(P(\text{nein})\)</span>, <span class="math inline">\(P(\text{Himmel=sonnig}|\text{ja})\)</span>, <span class="math inline">\(P(\text{Himmel=sonnig}|\text{nein})\)</span>, …</p>
<p>Wähle Klasse <span class="math display">\[
\begin{array}{rl}
h_{MAP} = \operatorname{argmax}_{h \in \{\text{ja, nein}\}} &amp; P(h) \cdot P(\text{Himmel=sonnig}|h) \cdot P(\text{Luft=kalt}|h) \\
    &amp; \cdot P(\text{Wasser=warm}|h) \cdot P(\text{Wind=windstill}|h)
\end{array}
\]</span></p>
<p><span class="bsp">Tafelbeispiel</span> </p>
<h2 id="naivität-im-naive-bayes">Naivität im Naive Bayes</h2>
<ul>
<li><p>Unabhängigkeit der Attribute oft nicht gegeben</p>
<p><span class="blueArrow">=&gt;</span> <span class="math inline">\(P(D_1, ..., D_n|H) \ne \prod_i P(D_i|H)\)</span></p></li>
<li><p>A-posteriori-Wahrscheinlichkeiten oft unrealistisch nah an 1 oder 0</p></li>
</ul>

<ul>
<li><p>Praxis: Dennoch häufig sehr gute Ergebnisse :-)</p>
<p>Wichtig: <span class="math inline">\(\operatorname{argmax}_{h \in H} P(h) P(D_1, ..., D_n|h) = \operatorname{argmax}_{h \in H} P(h) \prod_i P(D_i|h)\)</span></p>

<p>Solange die <strong>Maximierung</strong> die <em>selben Ergebnisse</em> liefert, müssen die konkreten Schätzungen/Werte nicht exakt stimmen …</p>
</li>
</ul>

<p>Wenn Attribute nicht (bedingt) unabhängig sind, kann sich der NB verschätzen, d.h. es kommt dann u.U. zu einer höheren Fehlerrate, da bestimmte Eigenschaften in der Trainingsmenge zu hoch gewichtet werden.</p>

<h2 id="probleme-mit-floating-point-underflow">Probleme mit Floating Point Underflow</h2>
<ul>
<li><p>MAP berechnet Produkt mit vielen Termen</p></li>
<li><p>Problem: Bei kleinen Zahlen kann <strong>Floating Point Underflow</strong> auftreten!</p></li>
</ul>

<ul>
<li><p>Lösung: Logarithmus maximieren (Produkt geht in Summe über)</p>

<p>Erinnerung: <span class="math inline">\(\log(x \cdot y) = \log(x) + \log(y)\)</span> und Logarithmus streng monoton</p>

<p><span class="math display">\[
\begin{array}{rcl}
h_{MAP} &amp;=&amp; \operatorname{argmax}_{h \in H} P(h|D_1, ..., D_n) \\[5pt]
        &amp;=&amp; \operatorname{argmax}_{h \in H} P(h) \prod_i P(D_i|h) \\[5pt]
        &amp;=&amp; \operatorname{argmax}_{h \in H} [\log(P(h)) + \sum_i \log(P(D_i|h))]
\end{array}
\]</span></p></li>
</ul>
<h2 id="laplace-schätzer">Laplace-Schätzer</h2>
<ul>
<li>Problem: Attribut-Ausprägung für bestimmte Klasse nicht in Trainingsmenge:
<ul>
<li><span class="blueArrow">=&gt;</span> Bedingte Wahrscheinlichkeit ist 0</li>
<li><span class="blueArrow">=&gt;</span> Produkt gleich 0</li>
</ul></li>
</ul>

<ul>
<li><p>Lösung: “Laplace-Schätzer” (auch “Laplace-Glättung”) </p>


<p>Statt <span class="math inline">\(P(D_i=x|h) = \dfrac{|S_{D_i}(x) \cap S(h)|}{|S(h)|}\)</span></p>


<p>nutze <span class="math inline">\(P(D_i=x|h) = \dfrac{|S_{D_i}(x) \cap S(h)| + m \cdot p_i}{|S(h)| + m}\)</span></p>

<ul>
<li>mit <span class="math inline">\(m\)</span>: frei wählbarer Faktor, und</li>
<li><p><span class="math inline">\(p_i\)</span>: A-priori-Wahrscheinlichkeit für <span class="math inline">\(P(D_i=x|h)\)</span></p>

<p>Hintergrundwissen oder einfach <em>uniforme Verteilung der Attributwerte</em>: <span class="math inline">\(p_i = 1/|D_i|\)</span> (Wahrscheinlichkeit für eine Attributausprägung ist 1/(Anzahl der Ausprägungen des Attributs))</p>
</li>
</ul>
<p><span class="blueArrow">=&gt;</span> “virtuelle” Trainingsbeispiele (<span class="math inline">\(m\)</span> ist die Zahl der virtuellen Trainingsbeispiele) </p></li>
</ul>
<h2 id="maximum-likelihood">Maximum Likelihood</h2>
<ul>
<li><strong>Maximum a Posteriori</strong> <span class="math display">\[
h_{MAP} = \operatorname{argmax}_{h \in H} P(h|D_1, ..., D_n)
= \operatorname{argmax}_{h \in H} P(h) \prod_i P(D_i|h)
\]</span></li>
</ul>

<ul>
<li><p>Annahme: Klassen uniform verteilt <span class="blueArrow">=&gt;</span> <span class="math inline">\(P(h_i) = P(h_j)\)</span></p>
<p><strong>Maximum Likelihood</strong> <span class="math display">\[
h_{ML} = \operatorname{argmax}_{h \in H} \prod_i P(D_i|h)
\]</span></p>
<p><span class="blueArrow">=&gt;</span> Maximiere die Likelihood der Daten</p></li>
</ul>

<h2 id="ausblick-kontinuierliche-attribute">Ausblick: Kontinuierliche Attribute</h2>
<p>Bisher sind wir von diskreten Attributen ausgegangen. Bei kontinuierlichen Attributen hat man zwei Möglichkeiten:</p>
<ul>
<li><p>Diskretisierung der Attribute: Aufteilung in Intervalle und Bezeichnung der Intervalle mit einem Namen</p></li>
<li><p>Einsatz einer Verteilungsannahme und deren Dichtefunktion, beispielsweise Annahme von <strong>normalverteilten</strong> Daten mit der Dichtefunktion <span class="math display">\[
f(x) = \frac{1}{\sqrt{2 \pi \sigma}} e^{- \frac{(x - \mu)^2}{2 \sigma^2}}
\]</span></p>
<p>wobei <span class="math inline">\(\mu\)</span> der Mittelwert und <span class="math inline">\(\sigma^2\)</span> die Varianz der Daten sind.</p></li>
</ul>


<h2 id="ausblick-textklassifikation-mit-nb">Ausblick: Textklassifikation mit NB</h2>
<ul>
<li>Texte als Trainingsmenge:
<ul>
<li>Text zerlegen in Terme (Wörter, sonstige relevante Token)</li>
<li>ggf. Entfernen von Stoppwörtern (beispielsweise Artikel u.ä.)</li>
<li>ggf. Stemming und Lemmatisierung für restliche Terme</li>
<li>ggf. weitere Vorverarbeitungsschritte (Groß-Klein-Schreibung, …)</li>
<li>Terme eines Textes zusammenfassen als Menge: <em>“Bag of Words”</em></li>
</ul></li>
<li>Naive Bayes:
<ul>
<li><p>A-priori-Wahrscheinlichkeit der Klassen: <span class="math inline">\(P(c) = \dfrac{N_c}{N} = \dfrac{\text{Anzahl Dokumente in Klasse c}}{\text{Anzahl Dokumente}}\)</span></p></li>
<li><p>Likelihood der Daten (Terme): <span class="math inline">\(P(t|c) = \dfrac{T_{ct}}{\sum_{v \in V} T_{vt}}\)</span></p>
<p>mit <span class="math inline">\(T_{ct}\)</span> Anzahl der Vorkommen von Term <span class="math inline">\(t\)</span> in allen Dokumenten der Klasse <span class="math inline">\(c\)</span> und <span class="math inline">\(V\)</span> die Vereinigung aller Terme aller Dokumente (als Menge)</p>
<p>Variante mit Laplace-Glättung: <span class="math inline">\(P(t|c) = \dfrac{T_{ct} + 1}{\sum_{v \in V} T_{vt} + |V|}\)</span></p></li>
</ul></li>
</ul>


<p><em>Hinweis</em> zum Sprachgebrauch: In Abhängigkeit von der Verteilung der <span class="math inline">\(P(D_i|h)\)</span> spricht man von</p>
<ul>
<li>“multinominalem” NB: Attribute umfassen mehrere Kategorien (verschiedene Ausprägungen, wie im “Soll ich schwimmen gehen”-Beispiel oben)</li>
<li>Bernoulli NB: Attribute sind binär (Ausprägung 0 oder 1), typischerweise bei der Textklassifikation</li>
<li>Gauss’sches NB: Annahme einer Normalverteilung der Attribut-Ausprägungen</li>
</ul>

<h1 id="zusammenfassung">Zusammenfassung</h1>
<h2 id="tldr">tl;dr</h2>
<ul>
<li><p>Grundlagen der Wahrscheinlichkeitstheorie</p>

<ul>
<li>Elementarereignisse und Wahrscheinlichkeit</li>
<li>Rechenregeln</li>
<li>Bedingte Wahrscheinlichkeit und Verbundwahrscheinlichkeit</li>
<li>Marginalisierung</li>
<li>(Bedingte) Unabhängigkeit</li>
<li>Bayes’sche Regel</li>
</ul>
</li>
<li><p>Klassifikation mit Naive Bayes</p>

<ul>
<li>Annahme von Unabhängigkeit <span class="blueArrow">=&gt;</span> “Naive” Bayes Klassifikation</li>
<li>Schätzen der bedingten Wahrscheinlichkeiten aus den Trainingsdaten</li>
<li>Klassifikation durch Nutzung der geschätzten Wahrscheinlichkeiten</li>
<li>Hinweis auf Naivität der Annahme, dennoch sehr gute Erfolge in Praxis</li>
<li>Hinweis auf Probleme mit niedrigen Wahrscheinlichkeiten</li>
</ul>
</li>
</ul>

<p><strong>Nächste Woche</strong>: Rückblick, Umfrage, Klausurvorbereitung</p>

<h2 id="literatur-zum-weiterlesen">Literatur zum Weiterlesen</h2>
<ul>
<li>Russell, Norvig: “Artificial Intelligence: A Modern Approach, 3rd ed.”
<ul>
<li>Grundlagen: Kapitel 13</li>
<li>Klassifikation mit Naive Bayes: Kapitel 20.1, 20.2</li>
<li>Bayes’sche Netze: Kapitel 14</li>
</ul></li>
<li>Wolfgang Ertel: “Grundkurs Künstliche Intelligenz”, Springer Vieweg, 2016
<ul>
<li>Grundlagen: Kapitel 7</li>
<li>Klassifikation mit Naive Bayes: Kapitel 8.7</li>
</ul></li>
</ul>
<h2 id="lernziele-nach-dieser-vorlesung-sollten-sie">Lernziele – Nach dieser Vorlesung sollten Sie …</h2>
<h3 id="anwenden-k3">Anwenden (K3)</h3>
<ul>
<li>Grundlagen der Wahrscheinlichkeitstheorie
<ul>
<li>Elementarereignisse und Wahrscheinlichkeit</li>
<li>Rechenregeln</li>
<li>Bedingte Wahrscheinlichkeit und Verbundwahrscheinlichkeit</li>
<li>Marginalisierung</li>
<li>(Bedingte) Unabhängigkeit</li>
<li>Bayes’sche Regel</li>
</ul></li>
<li>Klassifikation mit Naive Bayes
<ul>
<li>Annahme von Unabhängigkeit <span class="blueArrow">=&gt;</span> “Naive” Bayes Klassifikation</li>
<li>Schätzen der bedingten Wahrscheinlichkeiten aus den Trainingsdaten</li>
<li>Klassifikation durch Nutzung der geschätzten Wahrscheinlichkeiten</li>
</ul></li>
<li>Probleme und Verbesserungen mit/bei Naive Bayes
<ul>
<li>Naivität der Annahme, dennoch sehr gute Erfolge in Praxis</li>
<li>Probleme mit niedrigen Wahrscheinlichkeiten</li>
</ul></li>
</ul>
<h2 id="diese-fragen-sollten-sie-beantworten-können">Diese Fragen sollten Sie beantworten können …</h2>
<h3 id="wahrscheinlichkeitstheorie">Wahrscheinlichkeitstheorie</h3>
<ul>
<li><p>Wie hoch ist die Wahrscheinlichkeit, bei einem normalen 6-seitigen Spielwürfel eine gerade Zahl zu würfeln? Wie hoch, eine Zahl kleiner 5 zu würfeln?</p></li>
<li><p>Was bedeutet eine bedingte Wahrscheinlichkeit?</p></li>
<li><p>Gegeben: 100 Fahrzeuge beobachtet, 30 davon wurden von Studierenden gefahren, bei insgesamt 10 war die Geschwindigkeit zu hoch (wobei davon 5 durch Studierende gesteuert wurden).</p>
<p>Gesucht: Wie hoch ist der Anteil der Fahrzeuge, die zu schnell unterwegs waren, unter der Bedingung, dass diese durch Studierende gesteuert wurden?</p></li>
<li><p>Was bedeutet Marginalisierung?</p></li>
<li><p>Gegeben das Karies-Zahnschmerzen-Beispiel:</p>
<table>
<thead>
<tr class="header">
<th align="left"></th>
<th align="left">Zahnschmerzen</th>
<th align="left"><span class="math inline">\(\neg\)</span> Zahnschmerzen</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">Karies</td>
<td align="left">0.04</td>
<td align="left">0.06</td>
</tr>
<tr class="even">
<td align="left"><span class="math inline">\(\neg\)</span> Karies</td>
<td align="left">0.01</td>
<td align="left">0.89</td>
</tr>
</tbody>
</table>
<p>Wie hoch ist die Wahrscheinlichkeit, keine Karies zu haben?</p></li>
<li><p>Wann sind Ereignisse unabhängig? Was bedeutet das?</p></li>
</ul>
<h3 id="naive-bayes-1">Naive Bayes</h3>
<ul>
<li><p>Warum ist der Bayes-Klassifikationsalgorithmus “naiv”? Wie zeigt sich das in der Praxis?</p></li>
<li><p>Welche Probleme gibt es bei der Anwendung des NB darüber hinaus zu beachten?</p></li>
<li><p>Warum kann bei NB der Normierungsfaktor <span class="math inline">\(P(D_1, ..., D_n)\)</span> weggelassen werden?</p></li>
<li><p>Wann wird NB zu ML?</p></li>
<li><p>Wie funktioniert Spam-Klassifikation mit NB?</p></li>
</ul>
<h3 id="diagnose-mit-nb">Diagnose mit NB</h3>
<p>Betrachten Sie folgende Trainingsmenge</p>
<table>
<thead>
<tr class="header">
<th align="left">Nase läuft</th>
<th align="left">Husten</th>
<th align="left">Gerötete Haut</th>
<th align="left">Fieber</th>
<th align="left">Klasse</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">1</td>
<td align="left">1</td>
<td align="left">1</td>
<td align="left">0</td>
<td align="left">krank</td>
</tr>
<tr class="even">
<td align="left">1</td>
<td align="left">1</td>
<td align="left">0</td>
<td align="left">0</td>
<td align="left">krank</td>
</tr>
<tr class="odd">
<td align="left">0</td>
<td align="left">0</td>
<td align="left">1</td>
<td align="left">1</td>
<td align="left">krank</td>
</tr>
<tr class="even">
<td align="left">1</td>
<td align="left">0</td>
<td align="left">0</td>
<td align="left">0</td>
<td align="left">gesund</td>
</tr>
<tr class="odd">
<td align="left">0</td>
<td align="left">0</td>
<td align="left">0</td>
<td align="left">0</td>
<td align="left">gesund</td>
</tr>
<tr class="even">
<td align="left">0</td>
<td align="left">1</td>
<td align="left">1</td>
<td align="left">0</td>
<td align="left">gesund</td>
</tr>
</tbody>
</table>
<p>Trainieren Sie mit diesem Datensatz einen Naive Bayes Klassifikator. Verwenden Sie Laplace-Glättung mit <span class="math inline">\(m=4\)</span> und gehen Sie von einem uniformen Prior <span class="math inline">\(p\)</span> aus. Geben Sie alle (bedingten) Wahrscheinlichkeiten an.</p>
<p>Testen Sie alle Trainingsbeispiele mit dem NB-Klassifikator.</p>
<p>Wenden Sie den NB-Klassifikator auf neue Beispiele an:</p>
<ul>
<li>Person mit Husten und Fieber</li>
<li>Person mit laufender Nase und Fieber</li>
<li>Person mit laufender Nase und geröteter Haut</li>
</ul>
<p>Ergibt sich die selbe Klassifikation, wenn Sie mit ML (Maximum Likelihood) arbeiten?</p>
<h3 id="diagnose-mit-nb-nicht-binäre-attribute">Diagnose mit NB (nicht-binäre Attribute)</h3>
<p>Betrachten Sie folgende Trainingsmenge</p>
<table>
<thead>
<tr class="header">
<th align="left">Fieber</th>
<th align="left">Erbrechen</th>
<th align="left">Durchfall</th>
<th align="left">Schüttelfrost</th>
<th align="left">Klasse</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">nein</td>
<td align="left">nein</td>
<td align="left">nein</td>
<td align="left">nein</td>
<td align="left">gesund</td>
</tr>
<tr class="even">
<td align="left">mittel</td>
<td align="left">nein</td>
<td align="left">nein</td>
<td align="left">nein</td>
<td align="left">Grippe</td>
</tr>
<tr class="odd">
<td align="left">hoch</td>
<td align="left">nein</td>
<td align="left">nein</td>
<td align="left">ja</td>
<td align="left">Grippe</td>
</tr>
<tr class="even">
<td align="left">hoch</td>
<td align="left">ja</td>
<td align="left">ja</td>
<td align="left">nein</td>
<td align="left">Salmonellen</td>
</tr>
<tr class="odd">
<td align="left">mittel</td>
<td align="left">nein</td>
<td align="left">ja</td>
<td align="left">nein</td>
<td align="left">Salmonellen</td>
</tr>
<tr class="even">
<td align="left">nein</td>
<td align="left">ja</td>
<td align="left">ja</td>
<td align="left">nein</td>
<td align="left">Magen-Darm</td>
</tr>
<tr class="odd">
<td align="left">mittel</td>
<td align="left">ja</td>
<td align="left">ja</td>
<td align="left">nein</td>
<td align="left">Magen-Darm</td>
</tr>
</tbody>
</table>
<p>Trainieren Sie mit diesem Datensatz einen Naive Bayes Klassifikator. Verwenden Sie Laplace-Glättung mit <span class="math inline">\(m=6\)</span> und gehen Sie von einem uniformen Prior <span class="math inline">\(p\)</span> aus. Geben Sie alle (bedingten) Wahrscheinlichkeiten an.</p>
<p>Wenden Sie den NB-Klassifikator auf neue Beispiele an:</p>
<ul>
<li>Person mit hohem Fieber</li>
<li>Person mit Erbrechen und Schüttelfrost</li>
</ul>
<p>Ergibt sich die selbe Klassifikation, wenn Sie mit ML (Maximum Likelihood) arbeiten?</p>
<h3 id="textklassifikation-mit-nb-1">Textklassifikation mit NB</h3>
<ul>
<li>Trainingsmenge:
<ul>
<li>D1: (“Sieben Zwerge fraßen sieben Ziegen”, 1)</li>
<li>D2: (“Sieben Ziegen traten sieben Wölfe”, 0)</li>
<li>D3: (“Sieben Wölfe fraßen sieben Böcke”, 1)</li>
<li>D4: (“Sieben Böcke traten sieben Zwerge”, 0)</li>
</ul></li>
<li>Testmenge:
<ul>
<li>T1: (“Sieben Zwerge fraßen sieben Wölfe”, 1)</li>
<li>T2: (“Sieben Zwerge traten sieben Ziegen”, 0)</li>
</ul></li>
</ul>
<p>Lernen Sie mit Hilfe der Trainingsmenge einen Naive-Bayes-Klassifikator und wenden Sie diesen auf die beiden Test-Dokumente an.</p>
<p>Ergibt sich die selbe Klassifikation, wenn Sie mit ML (Maximum Likelihood) arbeiten?</p>
</body>
</html>
